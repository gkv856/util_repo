{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A4D4ua_ODoI"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9r6g0jPKaE9"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader as DL\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmGqxpAuXKXQ"
      },
      "source": [
        "#HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg8YGX3SXLze"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "max_len = 16\n",
        "num_heads = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhAVQnnGD6y1"
      },
      "source": [
        "#Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EywhlOXTD9yy",
        "outputId": "967dc4f5-12cc-4bcf-9ddf-832667bdb3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive as gdrive\n",
        "gdrive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSGjdCuYD-Mi",
        "outputId": "cfc460f7-b9d5-4e55-f926-785326d019b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chameleons.pdf\t\tmovie_characters_metadata.txt  pairs_encoded.json\n",
            "checkpoint_9.pth\tmovie_conversations.txt        raw_script_urls.txt\n",
            "checkpoint_final_9.pth\tmovie_lines.txt\t\t       README.txt\n",
            "checkpoint_final9.pth\tmovie_titles_metadata.txt      WORDMAP_corpus.json\n"
          ]
        }
      ],
      "source": [
        "# Change directory and list files\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/AI/cornell movie-dialogs corpus/\")\n",
        "!ls  # List files in the current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh-KSJkOEE8y"
      },
      "outputs": [],
      "source": [
        "corpus_movie_conv = '/content/drive/My Drive/AI/cornell movie-dialogs corpus/movie_conversations.txt'\n",
        "corpus_movie_lines = '/content/drive/My Drive/AI/cornell movie-dialogs corpus/movie_lines.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmFlpj8jPbKk"
      },
      "source": [
        "#Data Prepration\n",
        "\n",
        "- In our approach, we establish a fixed length for our sequences, ensuring consistency in our data processing.\n",
        "- As we handle data in batches, it's crucial to determine this maximum length beforehand.\n",
        "- By doing so, we can efficiently store our data in matrices,\n",
        "streamlining the input process for our neural network.\n",
        "- To accommodate sentences shorter than the designated maximum length, we employ padding.\n",
        "- In this instance, we've set the maximum length at 25 characters, providing a standardized framework for our data processing pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PvQyxfdPWmW"
      },
      "source": [
        "##Reading the Movie Conversation and Lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq7PkBZUQkNs"
      },
      "source": [
        "##Understanding coversations\n",
        "\n",
        "### Conversation Grouping\n",
        "- The conversation data is structured such that consecutive lines form coherent conversations.\n",
        "- Each group of lines represents a single conversation.\n",
        "\n",
        "### Example\n",
        "- For instance, lines 194 to 197 constitute one conversation.\n",
        "- Similarly, lines 198 and 199 form another conversation.\n",
        "\n",
        "This grouping approach facilitates the analysis and processing of conversations within the dataset, enabling efficient handling of sequential dialogues.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# file_path = 'path/to/your/file.txt'\n",
        "\n",
        "# if os.path.exists(file_path):\n",
        "#     print(\"File exists.\")\n",
        "# else:\n",
        "#     print(\"File does not exist.\")\n"
      ],
      "metadata": {
        "id": "Aq-ubPCmc8Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJEYMxJ9OU2p"
      },
      "outputs": [],
      "source": [
        "with open(corpus_movie_conv, 'r') as c:\n",
        "    conv = c.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdkExu4HPqr_"
      },
      "outputs": [],
      "source": [
        "# conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP_R3FScRFkk"
      },
      "source": [
        "##Understanding Lines\n",
        "\n",
        "### Explanation of Line Content\n",
        "- Each line in the dataset corresponds to a specific utterance within a conversation.\n",
        "- The content of each line includes the actual saying, either a question or a reply, along with the associated character.\n",
        "\n",
        "### Example Illustration\n",
        "- Line number 1045 contains the saying \"they do not.\"\n",
        "- The subsequent line provides the continuation of the conversation.\n",
        "- For instance, if we examine the first conversation:\n",
        "  - The initial line represents the question posed.\n",
        "  - The following line serves as the reply to that question.\n",
        "  - This pattern continues throughout the conversation.\n",
        "- To access a specific question, one can refer to the line number corresponding to the start of that question.\n",
        "- Similarly, the subsequent line contains the reply to the preceding question.\n",
        "\n",
        "This organization of the dataset enables easy identification and extraction of both questions and replies within the conversations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1oWt4cGPF9M"
      },
      "outputs": [],
      "source": [
        "with open(corpus_movie_lines, 'r', encoding='ISO-8859-1') as l:\n",
        "    lines = l.readlines()\n",
        "\n",
        "# lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnNMFSeGRnYo"
      },
      "source": [
        "##Data to dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv63b6dUR9rA",
        "outputId": "ca6da46a-22d6-49fc-c2e4-d0206607fec6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L1045', 'u0', 'm0', 'BIANCA', 'They do not!\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "lines[0].split(\" +++$+++ \")\n",
        "# we need index and what was said"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yi9uApsPIEP"
      },
      "outputs": [],
      "source": [
        "lines_dic = {}\n",
        "for line in lines:\n",
        "    objects = line.split(\" +++$+++ \")\n",
        "    line_idx = objects[0]\n",
        "    lines_dic[line_idx] = objects[-1]\n",
        "\n",
        "# lines_dic[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TdMCzpvORtOe",
        "outputId": "50742a26-c0d5-40d6-f637-bcdbbfc23bcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "lines_dic[\"L197\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P0PKBSeSfFP"
      },
      "source": [
        "##Cleaning the conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-qUaxfPPmuY"
      },
      "outputs": [],
      "source": [
        "def remove_punc(string):\n",
        "    \"\"\"\n",
        "    Remove punctuation characters from the input string and convert it to lowercase.\n",
        "\n",
        "    Parameters:\n",
        "    string (str): The input string containing punctuation characters.\n",
        "\n",
        "    Returns:\n",
        "    str: The input string without any punctuation characters and converted to lowercase.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define a string containing all punctuation characters\n",
        "    punctuations = '''!()-[]{};:\"\\<>/@#$%^&*_~'''\n",
        "\n",
        "    # Initialize an empty string to store the input string without punctuation\n",
        "    no_punct = \"\"\n",
        "\n",
        "    # Iterate over each character in the input string\n",
        "    for char in string:\n",
        "        # Check if the character is not a punctuation character\n",
        "        if char not in punctuations:\n",
        "            # Append the character to the string without punctuation\n",
        "            no_punct += char  # Space is also a character\n",
        "\n",
        "    # Convert the string without punctuation to lowercase and return it\n",
        "    return no_punct.lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtycIiwdSvGo"
      },
      "source": [
        "This code iterates over conversations in a dataset, extracting conversation IDs, and then creating question-answer pairs based on these IDs. It removes punctuation and leading/trailing whitespace from the lines corresponding to each ID, splits the lines into words, and limits the length of each to a specified maximum length. Finally, it appends the question-answer pair to a list of pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jYuWb4fnS9Ta",
        "outputId": "2a0a1008-8178-4b8c-cf6f-522ab6fa5f04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['L194', 'L195', 'L196', 'L197']\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# this is string and we need to convert this to a python list.\n",
        "conv[0].split(\" +++$+++ \")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HG6CZKVTRBu",
        "outputId": "cce44e52-d57f-4b93-c67a-70d61116a1a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L194', 'L195', 'L196', 'L197']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "eval(conv[0].split(\" +++$+++ \")[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7f9FlEjSh6U"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store question-answer pairs\n",
        "pairs = []\n",
        "\n",
        "# Iterate over each conversation in the dataset\n",
        "for i, con in enumerate(conv):\n",
        "    try:\n",
        "      # Extract the conversation IDs and evaluate them as a list\n",
        "      ids = eval(con.split(\" +++$+++ \")[-1])\n",
        "\n",
        "      # Iterate over the conversation IDs\n",
        "      for i in range(len(ids)):\n",
        "          # Initialize an empty list to store question-answer pairs for each conversation\n",
        "          qa_pairs = []\n",
        "\n",
        "          # Break the loop if it's the last conversation ID\n",
        "          if i == len(ids) - 1:\n",
        "              break\n",
        "\n",
        "          # Remove punctuation and leading/trailing whitespace from the lines corresponding to the conversation IDs\n",
        "          first = remove_punc(lines_dic[ids[i]].strip())\n",
        "          second = remove_punc(lines_dic[ids[i + 1]].strip())\n",
        "\n",
        "          # Split the lines into words and limit the length of each to 'max_len'\n",
        "          qa_pairs.append(first.split()[:max_len])\n",
        "          qa_pairs.append(second.split()[:max_len])\n",
        "\n",
        "          # Append the question-answer pair to the list of pairs\n",
        "          pairs.append(qa_pairs)\n",
        "    except:\n",
        "      print(\"Error on i =\", i, con)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "92WjklXiUQba",
        "outputId": "884629a1-9ac3-4d57-ecd3-20c11378f817"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "lines_dic[\"L194\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2VsYx-FSyiA"
      },
      "outputs": [],
      "source": [
        "question = lines_dic[\"L194\"].strip()\n",
        "reply = lines_dic[\"L195\"].strip()\n",
        "\n",
        "q_list = question.split()\n",
        "r_list = reply.split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EYvahFvUMnB"
      },
      "outputs": [],
      "source": [
        "# q_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvAyUYeZT9Q2"
      },
      "outputs": [],
      "source": [
        "# now qa pair is a 2d list\n",
        "qa_pair = [q_list, r_list]\n",
        "# qa_pair"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pairs = pairs[:1000]"
      ],
      "metadata": {
        "id": "XcvWIKCgsKN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQxTZhkUsIm3",
        "outputId": "b45d70f3-77fd-43cd-8ade-b689d6739695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221616"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jeq7Iv1sUafd"
      },
      "outputs": [],
      "source": [
        "# confirming that all the pairs have one 2 list\n",
        "for p in pairs:\n",
        "  if len(p) != 2:\n",
        "    print(len(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEygyJE4VAt6"
      },
      "source": [
        "##Word-to-Index Dictionary for Word Embeddings\n",
        "\n",
        "### Introduction\n",
        "Now we'll focus on constructing a word-to-index dictionary, an essential step in utilizing word embeddings. Word embeddings represent each word in a vocabulary as a dense vector, typically obtained from a one-hot encoding followed by an embedding layer. This process allows for more efficient representation and processing of textual data.\n",
        "\n",
        "### Process Overview\n",
        "- **Mapping Words to Indices**: Each unique word in the dataset will be assigned a unique index. This index will serve as the basis for creating one-hot vectors.\n",
        "- **Generating One-Hot Vectors**: PyTorch, our deep learning framework, will automatically convert these indices into one-hot vectors.\n",
        "- **Utilizing Embedding Layers**: The one-hot vectors will then be inserted into an embedding layer, which we'll explore in detail later. This layer transforms one-hot vectors into dense word embeddings, capturing semantic relationships between words.\n",
        "\n",
        "### Steps:\n",
        "1. **Collecting Unique Words**: The first step involves gathering all the unique words present in the datasets.\n",
        "2. **Calculating Word Frequencies**: We need to determine how often each word occurs in our dataset.\n",
        "3. **Filtering Low-Frequency Words**: Words that occur infrequently, less than five times for instance, will be removed. This helps streamline the vocabulary size and reduces the complexity of the output layer in our model.\n",
        "\n",
        "By following these steps, we ensure that our word-to-index dictionary effectively represents the vocabulary of our dataset while maintaining efficiency in computational resources.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXG0lPBWV4MD"
      },
      "source": [
        "##Creating Word Frequency Dictionary using collections\n",
        "This code iterates over each question-answer pair in the list of pairs and updates a Counter object called word_freq with the frequencies of words appearing in both the questions and answers. The update() method increments the counts for each word encountered in the pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LInpZJ-vVkSZ"
      },
      "outputs": [],
      "source": [
        "# Initialize a Counter object to store word frequencies\n",
        "word_freq = Counter()\n",
        "\n",
        "# Iterate over each question-answer pair in the list of pairs\n",
        "for pair in pairs:\n",
        "    # Update the word frequencies with the words from both the question and the answer\n",
        "    word_freq.update(pair[0])  # Update word frequencies with words from the question\n",
        "    word_freq.update(pair[1])  # Update word frequencies with words from the answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXDemI6uV7SV"
      },
      "outputs": [],
      "source": [
        "# word_freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KbNWJVaWSH1"
      },
      "source": [
        "##Filtering Words by Frequency:\n",
        "Words that occur less frequently than the specified threshold (`min_word_freq`) are filtered out from the word frequency dictionary (`word_freq`).\n",
        "\n",
        "**Creating Word-to-Index Mapping:**\n",
        "- Each remaining word is assigned a unique index in the `word_map` dictionary, starting from 1.\n",
        "- The index is incremented for each word in the list of filtered words, creating a word-to-index mapping.\n",
        "\n",
        "**Adding Special Tokens:**\n",
        "- Special tokens such as `<unk>` (unknown), `<start>` (start-of-sequence), `<end>` (end-of-sequence), and `<pad>` (padding) are added to the `word_map` dictionary with unique indices.\n",
        "- These tokens are crucial for data preprocessing and model training, allowing for handling of out-of-vocabulary words, marking sequence boundaries, and managing variable-length sequences.\n",
        "\n",
        "The resulting `word_map` dictionary provides a comprehensive mapping of words to indices, including special tokens, facilitating efficient data processing and model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyiwcZQ0Vq0r"
      },
      "outputs": [],
      "source": [
        "# Set the minimum word frequency threshold\n",
        "min_word_freq = 8\n",
        "\n",
        "# Filter words based on their frequency to exclude those occurring less frequently than the threshold\n",
        "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "\n",
        "# Create a word-to-index mapping dictionary\n",
        "word_map = {k: v + 1 for v, k in enumerate(words)}  # Assign unique indices to each word, starting from 1\n",
        "\n",
        "# Add special tokens to the word map with unique indices\n",
        "word_map['<unk>'] = len(word_map) + 1  # Unknown token for out-of-vocabulary words\n",
        "word_map['<start>'] = len(word_map) + 1  # Start-of-sequence token\n",
        "word_map['<end>'] = len(word_map) + 1  # End-of-sequence token\n",
        "word_map['<pad>'] = 0  # Padding token with index 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSHrUJMcWJLS"
      },
      "outputs": [],
      "source": [
        "# word_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOpracPqWvlX",
        "outputId": "7414eef5-62dd-4308-f1ee-5d6fb6b8e274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words are 17512.\n"
          ]
        }
      ],
      "source": [
        "print(\"Total words are {}.\".format(len(word_map)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlqM-0YhXcD6"
      },
      "source": [
        "##Saving the WordMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlAFeXMJVrPG"
      },
      "outputs": [],
      "source": [
        "with open('WORDMAP_corpus.json', 'w') as j:\n",
        "    json.dump(word_map, j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ1laJeHYFt7"
      },
      "source": [
        "## Encoding Words Using Word Mapping\n",
        "\n",
        "After creating the `word_map`, the next step is to encode the words using this mapping. Since neural networks require numerical inputs rather than strings, we need to represent words as indices in the `word_map`.\n",
        "\n",
        "### Function Definitions\n",
        "Two functions will be created for encoding: one for questions and one for replies.\n",
        "\n",
        "### Function: `encode_question`\n",
        "- **Input Arguments:**\n",
        "  - `words`: List of words in the question.\n",
        "  - `word_map`: Mapping of words to indices (`word_map`).\n",
        "\n",
        "- **Explanation:**\n",
        "  - This function, `encode_question`, converts each word in the question into its corresponding index using the provided `word_map`.\n",
        "\n",
        "### Function: `encode_reply`\n",
        "- **Input Arguments:**\n",
        "  - `words`: List of words in the reply.\n",
        "  - `word_map`: Mapping of words to indices (`word_map`).\n",
        "\n",
        "- **Explanation:**\n",
        "  - Similarly, the `encode_reply` function converts each word in the reply into its corresponding index using the `word_map`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSJnooaGXft9"
      },
      "outputs": [],
      "source": [
        "def encode_enc_inp(words, word_map):\n",
        "    \"\"\"\n",
        "    Encode a question into a sequence of indices using a word-to-index mapping.\n",
        "\n",
        "    Parameters:\n",
        "    words (list): List of words in the question.\n",
        "    word_map (dict): Mapping of words to indices.\n",
        "\n",
        "    Returns:\n",
        "    list: Encoded question as a sequence of indices.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert each word in the question to its corresponding index in the word map\n",
        "    # Use '<unk>' index for out-of-vocabulary words\n",
        "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words]\n",
        "\n",
        "    # Pad the encoded sequence with '<pad>' token to ensure uniform length\n",
        "    enc_c += [word_map['<pad>']] * (max_len - len(words))\n",
        "\n",
        "    return enc_c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f86i1T2vXuef"
      },
      "outputs": [],
      "source": [
        "def encode_dec_inp(words, word_map):\n",
        "    \"\"\"\n",
        "    Encode a reply into a sequence of indices using a word-to-index mapping.\n",
        "\n",
        "    Parameters:\n",
        "    words (list): List of words in the reply.\n",
        "    word_map (dict): Mapping of words to indices.\n",
        "\n",
        "    Returns:\n",
        "    list: Encoded reply as a sequence of indices.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert each word in the reply to its corresponding index in the word map\n",
        "    # Use '<unk>' index for out-of-vocabulary words\n",
        "    # Add '<start>' and '<end>' tokens to mark the start and end of the reply\n",
        "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n",
        "            [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n",
        "\n",
        "    return enc_c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRT9lRJNXwNZ"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store encoded question-answer pairs\n",
        "pairs_encoded = []\n",
        "\n",
        "# Iterate over each question-answer pair in the list of pairs\n",
        "for pair in pairs:\n",
        "    # Encode the question and the reply using the provided word-to-index mapping\n",
        "    qus = encode_enc_inp(pair[0], word_map)  # Encode the question\n",
        "    ans = encode_dec_inp(pair[1], word_map)  # Encode the reply\n",
        "\n",
        "    # Append the encoded question-answer pair to the list of encoded pairs\n",
        "    pairs_encoded.append([qus, ans])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzu6LKb4YZQ_",
        "outputId": "20a247b0-226c-48b1-dd16-0785f31b364e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[17509, 100, 17509, 4, 101, 53, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [17510, 103, 104, 39, 105, 106, 24, 17509, 17511, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "pairs_encoded[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaRqFeRtYv9o"
      },
      "source": [
        "##Saving Number coded WordMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n7f91HJY1Ja"
      },
      "outputs": [],
      "source": [
        "fname = \"pairs_encoded.json\"\n",
        "with open(fname, 'w') as p:\n",
        "    json.dump(pairs_encoded, p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP0Zqg7jZJRj"
      },
      "source": [
        "#Custom Dataset Class\n",
        "- Refer to this video if you are not sure how this works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl3xXG8jY1p7"
      },
      "outputs": [],
      "source": [
        "class MovieDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch dataset class for loading encoded question-reply pairs.\n",
        "\n",
        "    Args:\n",
        "    -----\n",
        "    None.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    pairs (list): List of encoded question-reply pairs.\n",
        "    dataset_size (int): Total number of question-reply pairs in the dataset.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    __init__(): Initializes the dataset by loading encoded pairs from a JSON file.\n",
        "    __getitem__(i): Retrieves the encoded question-reply pair at index i.\n",
        "    __len__(): Returns the total number of question-reply pairs in the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the dataset by loading encoded pairs from a JSON file.\n",
        "        Sets the total number of pairs in the dataset.\n",
        "        \"\"\"\n",
        "        self.pairs = json.load(open('pairs_encoded.json'))  # Load encoded pairs from a JSON file\n",
        "        self.dataset_size = len(self.pairs)  # Set the total number of pairs in the dataset\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        Retrieve the encoded question-reply pair at index i.\n",
        "\n",
        "        Args:\n",
        "        -----\n",
        "        i (int): Index of the pair to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple: Encoded question and reply tensors.\n",
        "        \"\"\"\n",
        "        # Convert the encoded question and reply to PyTorch LongTensors\n",
        "        enc_inp = torch.LongTensor(self.pairs[i][0])\n",
        "        dec = torch.LongTensor(self.pairs[i][1])\n",
        "\n",
        "        # Prepare Target Data\n",
        "        dec_inp = dec[ :-1]\n",
        "        dec_out = dec[1 : ]\n",
        "\n",
        "        return enc_inp, dec_inp, dec_out\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the total number of question-reply pairs in the dataset.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        int: Total number of pairs in the dataset.\n",
        "        \"\"\"\n",
        "        return self.dataset_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkrvIAD3ws8D"
      },
      "outputs": [],
      "source": [
        "train_data = MovieDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA_gtZ8U26f7",
        "outputId": "e1cf4ebc-c7fe-4ec5-adb2-d13ba3c32e41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([17509,   100, 17509,     4,   101,    53,   102,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0]),\n",
              " tensor([17510,   103,   104,    39,   105,   106,    24, 17509, 17511,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0]),\n",
              " tensor([  103,   104,    39,   105,   106,    24, 17509, 17511,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "q_r = train_data[10]\n",
        "q_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7OUrKvm3lQy"
      },
      "outputs": [],
      "source": [
        "rev_word_map = {v: k for k, v in word_map.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFjntuOm4ZXh"
      },
      "outputs": [],
      "source": [
        "def tensor_to_sentence(t, clean=False):\n",
        "  q = t.detach().numpy()\n",
        "  q_words = \" \".join([rev_word_map[v] for v in q])\n",
        "\n",
        "  if clean:\n",
        "    q_words = q_words.replace(\"<pad>\", \"\")\n",
        "\n",
        "  return q_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cDDhIV73xrz",
        "outputId": "70b014dc-c2c8-48d9-dfe6-37cb313a31b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<unk> ma <unk> this is my head <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>',\n",
              " \"<start> right. see? you're ready for the <unk> <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "q_words = tensor_to_sentence(q_r[0])\n",
        "r_words = tensor_to_sentence(q_r[1])\n",
        "q_words, r_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0VnECKDvvK0"
      },
      "source": [
        "#Custom Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYRqwS97vwhY"
      },
      "outputs": [],
      "source": [
        "train_loader = DL(train_data,\n",
        "                  batch_size = batch_size,\n",
        "                  shuffle=True,\n",
        "                  pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cTZIkMBZPkg",
        "outputId": "616952db-d574-4cb2-b98c-36867d8a5da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 16]) torch.Size([64, 17]) torch.Size([64, 17])\n",
            "he <unk> of not feeling well. i thought he was drunk  he <unk> <pad> <pad>\n",
            "<start> that <unk> his dying so quickly. in your <unk> have you never seen men who <unk>\n",
            "that <unk> his dying so quickly. in your <unk> have you never seen men who <unk> <end>\n"
          ]
        }
      ],
      "source": [
        "# the reason we have 25 length in question is because we defined max length as 25\n",
        "# reply has 25 + 2 = 27 because we have start and end appended to it\n",
        "# and of course there is padding if the sentence does not have 25 words in it.\n",
        "# for i, (enc_inp, dec_inp, dec,out) in enumerate(train_loader):\n",
        "\n",
        "for i, (enc_inp, dec_inp, dec_out) in enumerate(train_loader):\n",
        "  print(enc_inp.shape, dec_inp.shape, dec_out.shape)\n",
        "  print(tensor_to_sentence(enc_inp[0]))\n",
        "  print(tensor_to_sentence(dec_inp[0]))\n",
        "  print(tensor_to_sentence(dec_out[0]))\n",
        "\n",
        "  break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CdvqIEc1MCn"
      },
      "source": [
        "#Setting the device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYgYXAp91GaA",
        "outputId": "6cc1e990-cea9-46a3-d8e7-bcb290ebac08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ckm9dy7ze3b"
      },
      "source": [
        "#Mask\n",
        "\n",
        "This function **create_masks** generates masks for the *input question* and *reply* sequences to facilitate attention mechanisms in the neural network model.\n",
        "- It first defines a nested function subsequent_mask to create a mask preventing attending to subsequent positions.\n",
        "- Then, it creates masks for the input question, input reply, and target reply, ensuring proper masking for padding tokens and subsequent positions.\n",
        "- The masks are returned as a tuple for further use in the model.\n",
        "\n",
        "##Example\n",
        "- Sentence: `<start>Hello how are you <end>`\n",
        "- reply_input: `<start>Hello how are you`\n",
        "  - reply_input is input to our decoder\n",
        "- reply_target: `Hello how are you<end>`\n",
        "  - reply_target is the target to our decoder\n",
        "- Remember we are doing supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHWtQuDPDsse"
      },
      "outputs": [],
      "source": [
        "# # Batched scenario\n",
        "# t = torch.triu(torch.ones((2, 4, 4)))\n",
        "# t.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwTIyoXTEUNL"
      },
      "source": [
        "#Embeddings class with Positional Detail"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, pad_id):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size,\n",
        "                                            embed_size,\n",
        "                                            padding_idx=pad_id)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.token_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_embed = self.token_embedding(x)\n",
        "        return x_embed"
      ],
      "metadata": {
        "id": "TM_e_jHTG3W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    ref: https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/embedding/position.py\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "2c3feoXfG8Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqcSCIuC2A43"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab, embed_size, max_len):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size=len(vocab),\n",
        "                                              embed_size=embed_size,\n",
        "                                              pad_id=vocab[\"<pad>\"])\n",
        "        self.embed_size = embed_size\n",
        "        self.pos_embedding = PositionalEmbedding(d_model=embed_size,\n",
        "                                                 max_len=max_len+2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        token_embed = self.token_embedding(x) * math.sqrt(self.embed_size)\n",
        "        pos_embed = self.pos_embedding(x)\n",
        "\n",
        "        # print(x.shape, token_embed.shape, pos_embed.shape)\n",
        "\n",
        "        return token_embed + pos_embed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tji_5yTpIssa"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab,\n",
        "                 d_model=512,\n",
        "                 n_head=8,\n",
        "                 num_encoder_layers=6,\n",
        "                 num_decoder_layers=6,\n",
        "                 dim_feedforward=2048,\n",
        "                 dropout=0.1,\n",
        "                 max_len=15) -> None:\n",
        "        \"\"\"Instantiating Transformer class\n",
        "        Args:\n",
        "            config (Config): model config, the instance of data_utils.utils.Config\n",
        "            vocab (Vocabulary): the instance of data_utils.vocab_tokenizer.Vocabulary\n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        d_model = d_model #512\n",
        "        n_head = n_head #8\n",
        "        num_encoder_layers = num_encoder_layers #6\n",
        "        num_decoder_layers = num_decoder_layers #6\n",
        "        dim_feedforward = dim_feedforward #2048\n",
        "        dropout = dropout #0.1\n",
        "\n",
        "        self.input_embedding = Embeddings(vocab, d_model, max_len)\n",
        "\n",
        "        self.transfomrer = torch.nn.Transformer(d_model=d_model,\n",
        "                                                nhead=n_head,\n",
        "                                                num_encoder_layers=num_encoder_layers,\n",
        "                                                num_decoder_layers=num_decoder_layers,\n",
        "                                                dim_feedforward=dim_feedforward,\n",
        "                                                dropout=dropout,\n",
        "                                                batch_first=True)\n",
        "\n",
        "        self.proj_vocab_layer = nn.Linear(in_features=d_model,\n",
        "                                          out_features=len(vocab))\n",
        "\n",
        "        # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.apply\n",
        "        # self.apply(self._initailze)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.proj_vocab_layer.bias.data.zero_()\n",
        "        self.proj_vocab_layer.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, enc_input: torch.Tensor, dec_input: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x_enc_embed = self.input_embedding(enc_input.long())\n",
        "        x_dec_embed = self.input_embedding(dec_input.long())\n",
        "\n",
        "        # Masking\n",
        "        # tensor([[False, False, False,  True,  ...,  True]])\n",
        "        src_key_padding_mask = enc_input == self.vocab[\"<pad>\"]\n",
        "        tgt_key_padding_mask = dec_input == self.vocab[\"<pad>\"]\n",
        "\n",
        "        memory_key_padding_mask = src_key_padding_mask\n",
        "        tgt_mask = self.transfomrer.generate_square_subsequent_mask(dec_input.size(1))\n",
        "\n",
        "        # transformer ref: https://pytorch.org/docs/stable/nn.html#torch.nn.Transformer\n",
        "        src_key_padding_mask = src_key_padding_mask.type(torch.float)\n",
        "        tgt_key_padding_mask = tgt_key_padding_mask.type(torch.float)\n",
        "        memory_key_padding_mask = memory_key_padding_mask.type(torch.float)\n",
        "        tgt_mask = tgt_mask.type(torch.float).to(device)\n",
        "\n",
        "        feature = self.transfomrer(src = x_enc_embed,\n",
        "                                   tgt = x_dec_embed,\n",
        "                                   src_key_padding_mask = src_key_padding_mask,\n",
        "                                   tgt_key_padding_mask = tgt_key_padding_mask,\n",
        "                                   memory_key_padding_mask=memory_key_padding_mask,\n",
        "                                   tgt_mask = tgt_mask)\n",
        "\n",
        "        logits = self.proj_vocab_layer(feature)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_map[\"<pad>\"], len(word_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6HiplzZJOoX",
        "outputId": "496b75ea-0468-46b5-b13b-2ffbc0f7d660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 17512)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(word_map, max_len=15).to(device)\n",
        "for i, (enc_inp, dec_inp, dec_out) in enumerate(train_loader):\n",
        "  print(enc_inp.shape, dec_inp.shape, dec_out.shape)\n",
        "  enc_inp, dec_inp = enc_inp.to(device), dec_inp.to(device)\n",
        "  out = model(enc_inp, dec_inp)\n",
        "  print(out.shape, dec_out.shape)\n",
        "\n",
        "  # for 1 sentence form the batch\n",
        "  # we have (max_len, vocab_size) output\n",
        "  # hello - [vocab_size tensor with logit values]\n",
        "  # how - [vocab_size tensor with logit values]\n",
        "  # are - [vocab_size tensor with logit values]\n",
        "  # your - [vocab_size tensor with logit values]\n",
        "  # after softmax we will have 16 items with max values, we will compare that with dec_out\n",
        "  # and calcualte the loss\n",
        "  print(out[0].shape, dec_out[0].shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW31-A22I-H7",
        "outputId": "fbee57e2-5221-4a67-a893-da8fd7250583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 16]) torch.Size([64, 17]) torch.Size([64, 17])\n",
            "torch.Size([64, 17, 17512]) torch.Size([64, 17])\n",
            "torch.Size([17, 17512]) torch.Size([17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50vWNeLXEodK"
      },
      "source": [
        "#Creating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdgdplEvH0r9"
      },
      "source": [
        "#Optimizer Adam Warm Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHFO4Mu6F5Cd"
      },
      "outputs": [],
      "source": [
        "class AdamWarmup:\n",
        "\n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "\n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "\n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "\n",
        "\n",
        "        lr = self.get_lr()\n",
        "\n",
        "        # print(self.current_step, lr)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2znnQLqH6Sf"
      },
      "source": [
        "# Loss with Loss Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4H6SJwrZ2kr",
        "outputId": "fcde38fd-f5ea-49c9-92d2-681e83a91670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 26, 18243]) torch.Size([64, 26])\n",
            "Loss: 234.2530517578125\n"
          ]
        }
      ],
      "source": [
        "class LossWithLS(nn.Module):\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(reduction='batchmean' )\n",
        "\n",
        "        # self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "\n",
        "    def forward(self, x, target):\n",
        "      assert x.size(-1) == self.size\n",
        "      true_dist = torch.zeros_like(x.data)\n",
        "      true_dist.fill_(self.smooth / (self.size - 1))  # Fill with the smoothing value\n",
        "      true_dist.scatter_(2, target.unsqueeze(2), self.confidence)  # Assign the confidence value to the true index\n",
        "      true_dist = true_dist.detach()  # Detach true_dist from the computation graph\n",
        "\n",
        "      # return self.criterion(x, true_dist)\n",
        "      return self.criterion(F.log_softmax(x, dim=-1), true_dist)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "batch_size = 64\n",
        "max_words = 26\n",
        "vocab_size = 18243\n",
        "smooth = 0.1\n",
        "\n",
        "# Random tensors for demonstration\n",
        "prediction = torch.randn(batch_size, max_words, vocab_size)\n",
        "target = torch.randint(0, vocab_size, (batch_size, max_words))\n",
        "\n",
        "print(prediction.shape, target.shape)\n",
        "# Initialize and compute loss\n",
        "loss_fn = LossWithLS(size=vocab_size, smooth=smooth)\n",
        "loss = loss_fn(prediction, target)\n",
        "print(f'Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bRthKKpIDZv"
      },
      "source": [
        "#Evaluation of the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yty-gOIqdAXz"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, enc_inp, max_len, word_map):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    start_symbol = word_map['<start>']  # Assuming <sos> is the start-of-sequence token\n",
        "    end_symbol = word_map['<end>']  # Assuming <eos> is the end-of-sequence token\n",
        "\n",
        "    # Start with a target sequence of length 1 (just the start-of-sequence token)\n",
        "    dec_inp = torch.LongTensor([start_symbol]).unsqueeze(0).to(device)\n",
        "    # print(dec_inp.shape)\n",
        "\n",
        "    # Generate output iteratively\n",
        "    for i in range(max_len - 1):\n",
        "\n",
        "        # Calculate the output logits\n",
        "        output = model(enc_inp, dec_inp)\n",
        "        # print(output.shape)\n",
        "\n",
        "        # Get the last token from the output\n",
        "        next_token_logits = output[:, -1, :]\n",
        "\n",
        "        # Convert logits to probabilities and pick the token with the highest probability\n",
        "        next_token = next_token_logits.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "        # Append the predicted token to the target sequence\n",
        "        dec_inp = torch.cat([dec_inp, next_token], dim=1)\n",
        "\n",
        "        # Check if the end-of-sequence token was generated\n",
        "        if next_token.item() == end_symbol:\n",
        "            break\n",
        "\n",
        "    # Convert the target sequence to a list of tokens\n",
        "    tgt_tokens = dec_inp.squeeze(0).tolist()\n",
        "    # Convert tokens to words\n",
        "    sentence = ' '.join([reverse_word_map[token] for token in tgt_tokens if token not in (start_symbol, end_symbol)])\n",
        "\n",
        "    return sentence\n",
        "\n",
        "# Assuming you have a word_map and a reverse_word_map to convert between tokens and words\n",
        "reverse_word_map = {v: k for k, v in word_map.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghJdAuuzgPbc"
      },
      "outputs": [],
      "source": [
        "questions = [\"Hello how are you?\", \"I like Fruits\", \"Are you hungry?\"]\n",
        "def getResults(transformer, questions):\n",
        "  for q in questions:\n",
        "    enc_qus = [word_map.get(word, word_map['<unk>']) for word in q.split()]\n",
        "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
        "    # print(\"question.shape\", question.shape)\n",
        "    sentence = evaluate(transformer, question, max_len, word_map)\n",
        "    print(\"\\t\", sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(word_map, max_len=15).to(device)\n",
        "\n",
        "questions = [\"Hello how are you?\", \"I like Fruits\", \"Are you hungry?\"]\n",
        "\n",
        "getResults(transformer, questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdM9IDESY5PW",
        "outputId": "e2d8125d-dac3-4cf7-be6e-a0375e5a02d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t lamp ben. ben. ben. ben. ben. ben. ben. ben. ben. ben. ben. ben. ben. ben.\n",
            "\t interviewed interviewed interviewed interviewed interviewed interviewed interviewed interviewed interviewed interviewed interviewed interviewed momma momma momma\n",
            "\t lamp ben. lamp ben. ben. ben. ben. ben. ben. ben. ben. ben. ben. ben. ben.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gllno5BxH_83"
      },
      "source": [
        "#Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "dBlYEvUy0jTb",
        "outputId": "49344664-1e2f-4f9b-80ed-82b6c9755582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0][0/3463]\tLoss: 163.562\tLR: 0.00000\n",
            "\t park. horse fight running kentucky running kentucky alternative. kentucky gee, investigation? move, detachment amazing. particularly\n",
            "\t anybody's oz. file? deputy trade anybody's exit. powers, red gee, boyfriend's cohaagen kentucky witch, stay,\n",
            "\t professional. singer, crash. would... edward powwow crash. heard... fingers cigarette attached. deputy bullshit cigarette sleeping\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-ddec2f2c2d59>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtransformer_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m# print(loss.item(),  samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# d_model = 200\n",
        "# n_head = 2\n",
        "# num_encoder_layers = 2\n",
        "# num_decoder_layers = num_encoder_layers\n",
        "# dim_feedforward = 200\n",
        "# dropout = 0.2\n",
        "\n",
        "d_model = 512\n",
        "n_head = 2\n",
        "num_encoder_layers = 2\n",
        "num_decoder_layers = num_encoder_layers\n",
        "dim_feedforward = 512\n",
        "dropout = 0.2\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "transformer = Transformer(word_map,\n",
        "                 d_model=d_model,\n",
        "                 n_head=n_head,\n",
        "                 num_encoder_layers=num_encoder_layers,\n",
        "                 num_decoder_layers=num_decoder_layers,\n",
        "                 dim_feedforward=dim_feedforward,\n",
        "                 dropout=dropout,\n",
        "                 max_len=15).to(device)\n",
        "\n",
        "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size=512, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "criterion = LossWithLS(len(word_map), 0.1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, (enc_inp, dec_inp, dec_out) in enumerate(train_loader):\n",
        "\n",
        "        samples = enc_inp.shape[-1]\n",
        "\n",
        "        # Move to device\n",
        "        enc_inp = enc_inp.to(device)\n",
        "        dec_inp, dec_out = dec_inp.to(device), dec_out.to(device)\n",
        "\n",
        "        # Get the transformer outputs\n",
        "        out = transformer(enc_inp, dec_inp)\n",
        "\n",
        "        # Compute the loss\n",
        "        # print(out.shape, reply_target.shape)\n",
        "        loss = criterion(out, dec_out)\n",
        "\n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), 0.5)\n",
        "\n",
        "        transformer_optimizer.step()\n",
        "\n",
        "        sum_loss += loss.item() * samples\n",
        "        # print(loss.item(),  samples)\n",
        "        count += samples\n",
        "\n",
        "        if i % (batch_size * 5) == 0:\n",
        "            # print(loss.item(),  samples)\n",
        "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\\tLR: {:.5f}\".format(\n",
        "                epoch,\n",
        "                i,\n",
        "                len(train_loader),\n",
        "                sum_loss/count,\n",
        "                transformer_optimizer.lr))\n",
        "\n",
        "            getResults(transformer, questions)\n",
        "\n",
        "    # state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
        "    # torch.save(state, 'checkpoint_' + str(epoch) + '.pth.tar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmhOtN29NrCc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-A7wpN30v07"
      },
      "outputs": [],
      "source": [
        "state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
        "torch.save(state, 'checkpoint_final' + str(epoch) + '.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"Hello how are you?\",\n",
        "             \"Are you hungry?\",\n",
        "             \"How is life going?\",\n",
        "             \"I am sad\",\n",
        "             \"I kiss a girl\"]\n",
        "\n",
        "getResults(transformer, questions)"
      ],
      "metadata": {
        "id": "RiPlvqns9Mck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_optimizer"
      ],
      "metadata": {
        "id": "2x9TCCHOt8Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = {\n",
        "    'epoch': epoch,\n",
        "    'transformer_state_dict': transformer.state_dict(),\n",
        "    'transformer_optimizer_state_dict': transformer_optimizer.optimizer.state_dict()\n",
        "}\n",
        "torch.save(state, 'checkpoint_final_' + str(epoch) + '.pth')"
      ],
      "metadata": {
        "id": "6u_Lf_aFwnHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jkdP2VhNwnhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the checkpoint\n",
        "checkpoint = torch.load('checkpoint_final_9.pth')"
      ],
      "metadata": {
        "id": "yV0YWDwrwJ8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 200\n",
        "n_head = 2\n",
        "num_encoder_layers = 2\n",
        "num_decoder_layers = num_encoder_layers\n",
        "dim_feedforward = 200\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "transformer = Transformer(word_map,\n",
        "                 d_model=d_model,\n",
        "                 n_head=n_head,\n",
        "                 num_encoder_layers=num_encoder_layers,\n",
        "                 num_decoder_layers=num_decoder_layers,\n",
        "                 dim_feedforward=dim_feedforward,\n",
        "                 dropout=dropout,\n",
        "                 max_len=15).to(device)"
      ],
      "metadata": {
        "id": "RvIV5vhgKtL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restore the model and optimizer state\n",
        "transformer.load_state_dict(checkpoint['transformer_state_dict'])\n",
        "transformer_optimizer.optimizer.load_state_dict(checkpoint['transformer_optimizer_state_dict'])\n",
        "\n",
        "# Restore the last epoch\n",
        "start_epoch = checkpoint['epoch']\n"
      ],
      "metadata": {
        "id": "527z5ErqwKY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"Do you eat Fruits?\",\n",
        "             \"Lets go to France?\",\n",
        "             \"I am just happy\",\n",
        "             \"I kiss a girl\"]\n",
        "\n",
        "getResults(transformer, questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVFEOk6JKZWs",
        "outputId": "0e72014c-fdfe-498b-f1b5-947bc5add2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t i don't know what you're talking about.\n",
            "\t i don't want to see you again.\n",
            "\t i don't want to hear it.\n",
            "\t i don't know what to do with you, <unk>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}