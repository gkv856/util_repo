{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xieX2xhDO98d"
      },
      "source": [
        "#Installing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xTY6-SQrfu8X"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z14JDtkH-1qy"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1ITUFS_-WFo",
        "outputId": "a028e951-e965-48db-b88f-f143b1953404"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKamsp6AAmDG",
        "outputId": "9eeee6e4-b7ab-47b2-9d56-f5f704845477"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[31373]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "token_ids = tokenizer.encode(\"hello\", add_special_tokens=False)\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zl2zAj3XA372",
        "outputId": "c8fdbf44-9505-47f5-d543-5cae2b2741e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tokenizer.decode([31373])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiopDMuePLEi"
      },
      "source": [
        "#Downloading GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h9_pa72vPSLU"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYWaBO-N-59g",
        "outputId": "dcd12ea6-f4e9-4e98-d065-cf8dd4e1278b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# model = GPT2LMHeadModel.from_pretrained('gpt2-xl') # around 6.5gb\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2') # around 500mb\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28fOI2jsC1T-",
        "outputId": "e4eff69b-3f4f-4428-a4fe-5ccc4126323f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                                \"bos_token\": \"<startofstring>\",\n",
        "                                \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru_HDgTeVzwI",
        "outputId": "71b92bf2-7ddc-4a8a-d2d4-3c576782dac4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50261, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50261, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o-yhveUFcixM"
      },
      "outputs": [],
      "source": [
        "model_copy = copy.deepcopy(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHsK-CjYPb5a"
      },
      "source": [
        "#Connecting to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbRIfZwBrle9",
        "outputId": "e9c7fd05-b0d3-4dd9-921e-53ac89b2b825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive as gdrive\n",
        "\n",
        "MOUNT_DIR = '/content/drive'\n",
        "gdrive.mount(MOUNT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4q7O055WMaj",
        "outputId": "a467cc66-9ebd-4afe-c759-02660096d2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat_data.json\tgeneration_config.json\tmodel.safetensors\t tokenizer_config.json\n",
            "config.json\tmerges.txt\t\tspecial_tokens_map.json  vocab.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/My Drive/AI/fine-tuned-gpt2\"\n",
        "os.chdir(BASE_DIR)\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWeKsPqsPfh4"
      },
      "source": [
        "#Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0j2yXsJTN1lw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "\n",
        "class ChatData(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset class for handling chat data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str, tokenizer, max_sample=5000):\n",
        "        \"\"\"\n",
        "        Initialize the dataset by loading the data, formatting the dialogues,\n",
        "        and tokenizing the text.\n",
        "\n",
        "        Args:\n",
        "            path (str): The file path to the JSON data.\n",
        "            tokenizer: The tokenizer to be used for encoding the text data.\n",
        "        \"\"\"\n",
        "        # Load JSON data from the specified file path\n",
        "        with open(path, \"r\") as file:\n",
        "            self.data = json.load(file)\n",
        "\n",
        "        # Extract and format dialogues from the data\n",
        "        self.X = [j['text'] for i in self.data for j in i['dialog']]\n",
        "\n",
        "        # Format dialogues into the required string format\n",
        "        for idx in range(len(self.X) - 1):\n",
        "            self.X[idx] = f\"<startofstring> {self.X[idx]} <bot>: {self.X[idx + 1]} <endofstring>\"\n",
        "\n",
        "        # Limit the dataset to 5000 samples for manageability\n",
        "        self.max_sample = max_sample\n",
        "\n",
        "        self.X = self.X[:self.max_sample]\n",
        "\n",
        "        # Tokenize the dialogues using the provided tokenizer\n",
        "        self.X_encoded = tokenizer(self.X, max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        self.input_ids = self.X_encoded['input_ids']\n",
        "        self.attention_mask = self.X_encoded['attention_mask']\n",
        "\n",
        "        self.data_len = len(self.X)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of samples.\n",
        "        \"\"\"\n",
        "        return self.data_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return a single sample of the dataset given an index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing input_ids and attention_mask for the given index.\n",
        "        \"\"\"\n",
        "        return self.input_ids[idx], self.attention_mask[idx]\n",
        "\n",
        "    def print_original_text(self, idx):\n",
        "        \"\"\"\n",
        "        Print the original non-tokenized text of the dataset given an index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample to print.\n",
        "        \"\"\"\n",
        "        if 0 <= idx < self.data_len:\n",
        "            return self.X[idx]\n",
        "        else:\n",
        "            raise IndexError(\"Index out of range.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HlX-KfxDOHq_"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the ChatData class\n",
        "path_to_json  = \"chat_data.json\"\n",
        "ds_chat = ChatData(path=path_to_json,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_sample=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXsmNSt_N2TS",
        "outputId": "57195c71-0969-4ceb-b027-7c3881e847c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> /test <bot>: Text is not given. Please try to type /end and /test to reset the state and get text. <endofstring>\n",
            "(tensor([50258,  1220,  9288,   220, 50260,  8255,   318,   407,  1813,    13,\n",
            "         4222,  1949,   284,  2099,  1220,   437,   290,  1220,  9288,   284,\n",
            "        13259,   262,  1181,   290,   651,  2420,    13,   220, 50259, 50257,\n",
            "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
          ]
        }
      ],
      "source": [
        "idx = 28\n",
        "print(ds_chat.print_original_text(idx))\n",
        "print(ds_chat[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oljvIG4VRTMp"
      },
      "source": [
        "#Preparing Dataloader for Batch Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jk7uCGcTcgpV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "dl_chat = DataLoader(ds_chat,\n",
        "                     batch_size=batch_size,\n",
        "                     shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxmwtLfSEKBT",
        "outputId": "9c48107c-3cb0-4ef1-bb10-5a029c177b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 40]) torch.Size([64, 40])\n"
          ]
        }
      ],
      "source": [
        "for batch in dl_chat:\n",
        "    input_ids, attention_mask = batch\n",
        "    print(input_ids.shape, attention_mask.shape)\n",
        "    # Your training code here\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndjuN2VLRlvI"
      },
      "source": [
        "#Testing GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrRWueXK-O8X",
        "outputId": "de53f2a6-d444-4880-c26b-e07aefd6400d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[15496,    11,   616,  1438,   318,   402,    42,    53,   290,  3511,\n",
              "            30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "text = \"Hello, my name is GKV and yourself?\"\n",
        "encoded_input = tokenizer.encode_plus(\n",
        "    text,\n",
        "    return_tensors='pt',\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "encoded_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ixsaIvxWY8Y",
        "outputId": "880f8297-29e8-4de7-ebac-ee1410491198"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "pad_token_idx = tokenizer.encode(tokenizer.pad_token)[0]\n",
        "pad_token_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FECVVG8E_CVv"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "output_sequences = model.generate(\n",
        "    input_ids=encoded_input['input_ids'],\n",
        "    attention_mask=encoded_input['attention_mask'],\n",
        "    max_length=50,  # Maximum length of the generated text\n",
        "    num_return_sequences=1,  # Number of sequences to generate\n",
        "    no_repeat_ngram_size=2,  # Prevent repeating the same n-gram\n",
        "    top_k=50,  # Number of highest probability vocabulary tokens to keep for top-k-filtering\n",
        "    top_p=0.95,  # Cumulative probability of parameter highest probability vocabulary tokens\n",
        "    temperature=0.7,  # The value used to module the next token probabilities\n",
        "    do_sample=True,  # Set to True for sampling; False for greedy decoding\n",
        "    pad_token_id=pad_token_idx  # Setting pad token id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-3mv6JK_Ec-",
        "outputId": "efbecaad-f35c-40fd-972f-c3115bd7ce4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, my name is GKV and yourself? <bot>:  the – is a my name and me I who that you your what which it\n"
          ]
        }
      ],
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzOSH3rMR56x"
      },
      "source": [
        "#Train and Inference Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3jsWjqtCSMPW"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kAp2ohGFTvkA"
      },
      "outputs": [],
      "source": [
        "def infer(gptModel, tokenizer, inp):\n",
        "    \"\"\"\n",
        "    Generate a response from the model given an input string.\n",
        "\n",
        "    Args:\n",
        "        inp (str): The input string for which the model needs to generate a response.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response from the model.\n",
        "    \"\"\"\n",
        "    gptModel.eval()\n",
        "    # Prepare the input string by adding special tokens\n",
        "    inp = \"<startofstring> \" + inp + \" <bot>: \"\n",
        "\n",
        "    # Tokenize the input string and convert it to PyTorch tensors\n",
        "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "    X = inp[\"input_ids\"].to(device)\n",
        "    a = inp[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Generate the output using the model\n",
        "    output = gptModel.generate(X, attention_mask=a)\n",
        "\n",
        "    # Decode the output to get the generated response string\n",
        "    output = tokenizer.decode(output[0])\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iig3daihVT99"
      },
      "source": [
        "#Setting Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mAasM6xQVQgg",
        "outputId": "210ae77d-eb97-4971-f50e-00d3a3649861"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kb-l-neRrdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953cd9e4-69a8-4082-fa2d-d2a3c1c38334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Started .... \n",
            "Epoch: 1/10 \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tqdm\n",
        "\n",
        "\n",
        "total_batches = len(dl_chat)\n",
        "epochs = 10\n",
        "save = False\n",
        "\n",
        "model = copy.deepcopy(model_copy)\n",
        "model = model.to(device)\n",
        "optim = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Training Started .... \")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  running_loss = 0.0\n",
        "  model.train()\n",
        "\n",
        "  print(f\"Epoch: {epoch + 1}/{epochs} \")\n",
        "\n",
        "\n",
        "  # Iterate over batches\n",
        "\n",
        "  for batch_idx, (X, a) in enumerate(dl_chat):\n",
        "    # Move data to the device (GPU or CPU)\n",
        "    X = X.to(device)\n",
        "    a = a.to(device)\n",
        "\n",
        "    # Zero the parameter gradients\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X, attention_mask=a, labels=X)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    # Accumulate loss\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    # Print progress for each 10% completion\n",
        "    if (batch_idx + 1) % (total_batches // 10) == 0:\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    if batch_idx > 5:\n",
        "      break\n",
        "\n",
        "  if save:\n",
        "    # Save the model and tokenizer after each epoch with epoch information in the directory name\n",
        "    save_directory = f\"./fine-tuned-gpt2-epoch-{epoch + 1}\"\n",
        "    model.save_pretrained(save_directory)\n",
        "    tokenizer.save_pretrained(\"./tokenizer\")\n",
        "\n",
        "  # Print the loss for the current epoch and an inference example\n",
        "  avg_loss = running_loss / total_batches\n",
        "  print(f\"\\t Loss: {avg_loss:.4f}\", flush=True)\n",
        "  print(\"\")\n",
        "  print(infer(model, tokenizer, \"hello how are you\"))\n",
        "\n",
        "  break\n",
        "\n",
        "print(\"Training Completed .... \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf0mt67pXWHp",
        "outputId": "145b4dca-3c1f-4ba8-c1a6-1ad7dcfeb8b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Started .... \n",
            "Epoch: 1/10 \n"
          ]
        }
      ],
      "source": [
        "print(\"Training Started .... \")\n",
        "model = copy.deepcopy(model_copy)\n",
        "trained_gpt = train(dl_chat, model, tokenizer, device, epochs=10)\n",
        "print(\"Training Completed .... \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"infer from model : \")\n",
        "while True:\n",
        "  inp = input()\n",
        "  res = infer(trained_gpt, tokenizer, inp)\n",
        "  print(res)"
      ],
      "metadata": {
        "id": "vTeJGAsrYyur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pYSnKMQRvEZ"
      },
      "outputs": [],
      "source": [
        "# Load the SQuAD dataset\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Function to preprocess the dataset for GPT-2\n",
        "def preprocess_function(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    contexts = examples[\"context\"]\n",
        "    answers = [answer[\"text\"][0] for answer in examples[\"answers\"]]\n",
        "\n",
        "    # Concatenate question and context with special tokens\n",
        "    inputs = [f\"question: {question} context: {context} answer:\" for question, context in zip(questions, contexts)]\n",
        "    targets = [f\"{answer}\" for answer in answers]\n",
        "    return {\"input_texts\": inputs, \"target_texts\": targets}\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "tt_datasets = squad_dataset.map(preprocess_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UexmlWqeSqwa"
      },
      "outputs": [],
      "source": [
        "tt_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c68gLJAmRxcb"
      },
      "outputs": [],
      "source": [
        "def print_dataset_info(dataset):\n",
        "    print(f\"Dataset name: {dataset.__class__.__name__}\")\n",
        "    print(f\"Number of examples: {len(dataset)}\")\n",
        "    print(f\"Features: {dataset.features}\")\n",
        "    print(f\"Column names: {dataset.column_names}\")\n",
        "\n",
        "\n",
        "# Assuming 'tokenized_datasets' is your processed dataset from the previous steps\n",
        "print_dataset_info(tt_datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-R_o3bBSXfR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def display_random_examples(dataset, num_examples=5, tokenized=False):\n",
        "    random_indices = random.sample(range(len(dataset)), num_examples)\n",
        "    for idx in random_indices:\n",
        "        example = dataset[idx]\n",
        "        print(f\"Example {idx}:\")\n",
        "\n",
        "        if tokenized:\n",
        "           print(f\"Question: {example['input_ids']}\")\n",
        "           print(f\"Question: {example['labels']}\")\n",
        "        else:\n",
        "          print(f\"Question: {example['question']}\")\n",
        "          print(f\"Context: {example['context']}\")\n",
        "          print(f\"Answer: {example['answers']['text'][0]}\")\n",
        "        print()\n",
        "\n",
        "display_random_examples(tt_datasets[\"train\"], num_examples=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L5vo-rrV0sD"
      },
      "outputs": [],
      "source": [
        "max_length = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rgx5FUoUiAm"
      },
      "outputs": [],
      "source": [
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples[\"input_texts\"],\n",
        "                       truncation=True,\n",
        "                       padding=\"max_length\",\n",
        "                       max_length=max_length)\n",
        "\n",
        "    targets = tokenizer(examples[\"target_texts\"],\n",
        "                        truncation=True,\n",
        "                        padding=\"max_length\",\n",
        "                        max_length=max_length)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    target_ids = targets[\"input_ids\"]\n",
        "    return {\"input_ids\": input_ids, \"labels\": target_ids}\n",
        "\n",
        "# Tokenize the dataset\n",
        "rc = [\"input_texts\", \"target_texts\", \"context\", \"question\", \"answers\", \"id\", \"title\"]\n",
        "tokenized_datasets = tt_datasets.map(tokenize_function,\n",
        "                                            batched=True,\n",
        "                                            remove_columns=rc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWK40TZyW7Lh"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meXshcuMW0M5"
      },
      "outputs": [],
      "source": [
        "display_random_examples(tokenized_datasets[\"train\"],\n",
        "                        num_examples=3,\n",
        "                        tokenized=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2B1mfDOW4b4"
      },
      "outputs": [],
      "source": [
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hVrom8nbABu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuZ7Z-6JZLiH"
      },
      "outputs": [],
      "source": [
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"validation\"]\n",
        "batch_size = 64\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        return torch.tensor(item['input_ids']), torch.tensor(item['labels'])\n",
        "\n",
        "train_dataloader = DataLoader(QADataset(train_dataset),\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "eval_dataloader = DataLoader(QADataset(eval_dataset),\n",
        "                             batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sp0iw5bbhhU"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knTFD5DNbUZL"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJiCy2AcbFiX"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "# Inside the training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids, labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "        # Shift the labels to the right by one to match the GPT-2 training setup\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Training loss: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "    print(f\"Validation loss: {avg_eval_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3WI2ldqogjf"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./fine-tuned-gpt2\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-reIyIq0o5GU"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"./fine-tuned-gpt2\"\n",
        "\n",
        "loaded_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqFxus6Sqh4W"
      },
      "outputs": [],
      "source": [
        "model = copy.deepcopy(model_copy)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    train_steps = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids, labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_steps += 1\n",
        "\n",
        "    avg_train_loss = total_train_loss / train_steps\n",
        "    print(f\"Training loss: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    eval_steps = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            eval_steps += 1\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / eval_steps\n",
        "    print(f\"Validation loss: {avg_eval_loss}\")\n",
        "\n",
        "# End of training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqkkyaY9nUi0"
      },
      "outputs": [],
      "source": [
        "text = \"Hello how are you?\"\n",
        "# loaded_model = loaded_model.eval().to(device)\n",
        "model = model.eval().to(device)\n",
        "for i in range(2):\n",
        "  encoded_input = tokenizer.encode_plus(text,\n",
        "                                        return_tensors='pt',\n",
        "                                        padding=True,\n",
        "                                        truncation=True)\n",
        "\n",
        "\n",
        "  # Generate text\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=encoded_input['input_ids'].to(device),\n",
        "      attention_mask=encoded_input['attention_mask'].to(device),\n",
        "      max_length=50,  # Maximum length of the generated text\n",
        "      num_return_sequences=1,  # Number of sequences to generate\n",
        "      no_repeat_ngram_size=2,  # Prevent repeating the same n-gram\n",
        "      top_k=50,  # Number of highest probability vocabulary tokens to keep for top-k-filtering\n",
        "      top_p=0.95,  # Cumulative probability of parameter highest probability vocabulary tokens\n",
        "      temperature=0.9,  # The value used to module the next token probabilities\n",
        "      do_sample=True,  # Set to True for sampling; False for greedy decoding\n",
        "      pad_token_id=pad_token_id  # Setting pad token id\n",
        "  )\n",
        "\n",
        "  # Decode the generated text\n",
        "  generated_text = tokenizer.decode(output_sequences[0],\n",
        "                                    skip_special_tokens=True)\n",
        "\n",
        "  print(i, \"text\", text, \"\\tgenerated_text\", generated_text)\n",
        "\n",
        "  text += \" \" + generated_text\n",
        "\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODmy8hd2pyTP"
      },
      "outputs": [],
      "source": [
        "type(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EtvHqXFnU4J"
      },
      "outputs": [],
      "source": [
        "def generate_answer(question, context, model, tokenizer):\n",
        "    input_text = f\"question: {question} context: {context} answer:\"\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    encoded_input = tokenizer.encode_plus(\n",
        "    text,\n",
        "    return_tensors='pt',\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "\n",
        "    inputs = encoded_input\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    outputs = model.generate(input_ids=inputs['input_ids'].to(device),\n",
        "                              attention_mask=inputs['attention_mask'].to(device),\n",
        "                              max_length=512, num_return_sequences=1)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer.split(\"answer:\")[-1].strip()\n",
        "\n",
        "# Example usage\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"France is a country in Europe. The capital of France is Paris.\"\n",
        "answer = generate_answer(question, context, model, tokenizer)\n",
        "print(answer)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}